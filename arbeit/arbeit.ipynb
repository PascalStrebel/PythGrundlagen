{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Datenverteilung: {'1': 201688, '2': 151058, '3': 254295, '4': 585616, '5': 1807343}\n",
    "    --> Max 150'000 pro Klasse um Ungleichgewicht zu verhindern"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "{'1': 201688, '2': 151058, '3': 254295, '4': 585616, '5': 1807343}"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In order to test with a subset. Don't execute for real run\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/Books_rating.csv\", sep=\",\")\n",
    "counts = {\"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0}\n",
    "for index, row in data.iterrows():\n",
    "    reviewScore = '{0:g}'.format(row[\"review/score\"])\n",
    "    counts[reviewScore] = counts[reviewScore] + 1\n",
    "counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "Path(\"D:/nlp/1\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"D:/nlp/2\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"D:/nlp/3\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"D:/nlp/4\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"D:/nlp/5\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "counts = {\"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0}\n",
    "\n",
    "data = pd.read_csv(\"data/Books_rating.csv\", sep=\",\")\n",
    "for index, row in data.iterrows():\n",
    "    reviewScore = '{0:g}'.format(row[\"review/score\"])\n",
    "    counts[reviewScore] = counts[reviewScore] + 1\n",
    "    if (counts[reviewScore] > 150000):\n",
    "        continue\n",
    "    fp = open(\"D:/nlp/\" + reviewScore + \"/\" + str(index) + \".txt\", \"w\")\n",
    "    fp.write(row[\"review/text\"])\n",
    "    fp.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pasca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pasca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pasca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[81617 22813 11901  5872 12793]\n",
      " [32481 53064 28295 10064 11078]\n",
      " [15957 21525 57809 23562 16029]\n",
      " [ 9875  8191 29561 49100 38296]\n",
      " [11239  4980 11175 25726 81997]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.60      0.57    134996\n",
      "           1       0.48      0.39      0.43    134982\n",
      "           2       0.42      0.43      0.42    134882\n",
      "           3       0.43      0.36      0.39    135023\n",
      "           4       0.51      0.61      0.56    135117\n",
      "\n",
      "    accuracy                           0.48    675000\n",
      "   macro avg       0.48      0.48      0.47    675000\n",
      "weighted avg       0.48      0.48      0.47    675000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data = load_files(r\"D:/nlp\")\n",
    "X, y = data.data, data.target\n",
    "\n",
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    documents.append(document)\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(documents).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=0)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
